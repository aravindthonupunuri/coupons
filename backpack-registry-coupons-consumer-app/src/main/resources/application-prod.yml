micronaut:
  application:
    name: backpack-registry-coupons-consumer
  server:
    host: ${backpack-registry-coupons-consumer-host:localhost}
    port: 8085
  http:
    client.connect-ttl: 30m
    services:
      backpack-registry-api:
        urls:
          - "https://api.target.com"
        connect-timeout: 500ms
        read-timeout: 1000ms
        pool:
          enabled: true
  metrics:
    binders:
      web:
        enabled: false # we are using our own binder via our custom micronaut-metrics lib

filter:
  server:
    order:
      lists-brave-tracing-server-filter: 100
      list-authorization-filter: 200
      registry-channel-subchannel-filter: 400
  client:
    order:
      lists-brave-tracing-client-filter: 900
      resilience-client-filter: 800
      oauth-filter: 701
      oauth-key-filter: 700

components:
  server:
    list-authorization-filter:
      enabled: false
    sanitizing-filter:
      enabled: false
    registry-channel-subchannel-filter:
      enabled: false

msgbus:
  source: backpack-registry-coupons-prod
  dlq-source: backpack-registry-coupons-consumer-app-dlq-prod
  kafka:
    consumer:
      enabled: true
    producer:
      enabled: true
      client-id: backpack-registry-coupons-con-data-bus-prod-producer
    dlqconsumer:
      enabled: true
    dlqproducer:
      enabled: true
      client-id: backpack-registry-coupons-con-dlq-prod-producer
    topic: registry-internal-data-bus-prod
    consumer-group: backpack-registry-coupons-data-bus-prod-consumer
    consumer-batch-size: 100
    dlq-topic: registry-internal-data-bus-prod-dlq
    dlq-consumer-group: backpack-registry-coupons-data-bus-prod-dlq-consumer
    dlq-event-retry-interval-secs: 10800 # 3 hours, bring down post data migration
    dlq-max-event-retry-count: 16 # bring down post data migration
    dlq-consumer-batch-size: 50

notification:
  source: backpack-registry-coupons-prod
  dlq-source: backpack-registry-coupons-consumer-app-dlq-prod
  enabled: true

beacon:
  client:
    enabled: false
    source: backpack-registry-coupons-prod
    dlq-source: backpack-registry-coupons-consumer-app-dlq-prod
    kafka:
      consumer:
        client-id: backpack-registry-coupons-consumer-app-cron-beacon-prod-consumer
        topic: backpack-beacon-prod
        metrics-name: backpack-registry-coupons-consumer-app-cron-beacon-prod-metrics
        consumer-group: backpack-registry-coupons-consumer-app-cron-beacon-prod
        consumer-batch-size: 1
        max-count-down-latch-wait-time: 240 # in secondsConsumerRecord

kafka-sources:
  allow:
    - cronbeacon
    - backpack-registry-prod
    - backpack-transactions-prod

kafka:
  bootstrap:
    servers: ${kafkaenv.servers}
  ssl:
    endpoint.identification.algorithm: ""# disable karka broker cert's hostname verification
    keystore:
      location: /lists-bus-keystore.jks
      password: ${kafka-secret.keystore-password}
    truststore:
      location: /client-truststore.jks
      password: ${kafka-secret.truststore-password}
      type: PKCS12
  security:
    protocol: ssl
  consumers:
    backpack-registry-coupons-data-bus-prod-consumer:
      key:
        deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value:
        deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer
    backpack-registry-coupons-data-bus-prod-dlq-consumer:
      max.poll.interval.ms: 10980000 # 3.05 hours, bring down post data migration
      key:
        deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value:
        deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer
    backpack-registry-coupons-consumer-app-cron-beacon-prod:
      max.poll.interval.ms: 3780000 # 1.05 hours so kafka doesn't re-balance in the middle and processing won't complete
      key:
        deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value:
        deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer
  producers:
    # default is a Jackson based JSON serializer for key/value
    backpack-registry-coupons-con-data-bus-prod-producer:
      key:
        serializer: org.apache.kafka.common.serialization.StringSerializer
      value:
        serializer: com.tgt.lists.msgbus.ListsJsonSerde
      compression:
        type: zstd
      retries: 3
      retry:
        backoff:
          ms: 1000
      max:
        in:
          flight:
            requests:
              per:
                connection: 1
        block:
          ms: 2000
    notification-tracer-producer:
      key:
        serializer: org.apache.kafka.common.serialization.StringSerializer
      value:
        serializer: com.tgt.lists.msgbus.ListsJsonSerde
      compression:
        type: zstd
      retries: 3
      retry:
        backoff:
          ms: 1000
      max:
        in:
          flight:
            requests:
              per:
                connection: 1
        block:
          ms: 2000
    backpack-registry-coupons-con-dlq-prod-producer:
      key:
        serializer: org.apache.kafka.common.serialization.StringSerializer
      value:
        serializer: com.tgt.lists.msgbus.ListsJsonSerde
      compression:
        type: zstd
      retries: 3
      retry:
        backoff:
          ms: 1000
      max:
        in:
          flight:
            requests:
              per:
                connection: 1
        block:
          ms: 2000

postgres:
  max-db-pool-size: 5
  min-db-pool-size: 1
